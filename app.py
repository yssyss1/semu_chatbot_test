__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os
import time
import streamlit as st
from langchain_community.document_loaders import DirectoryLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.base import BaseCallbackHandler

os.environ["TOKENIZERS_PARALLELISM"] = "false"

st.set_page_config(page_title="ì„¸ë¬´ ì±—ë´‡", page_icon="ğŸ’¼", layout="centered")
st.title("ì„¸ë¬´ ìƒë‹´ ì±—ë´‡ í…ŒìŠ¤íŠ¸")

if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

if "vectorstore_loaded" not in st.session_state:
    st.session_state["vectorstore_loaded"] = False

if "is_loading" not in st.session_state:
    st.session_state["is_loading"] = False

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")

with st.spinner("ì±—ë´‡ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):

    vectorstore_dir = "chroma_vectorstore"

    model_name = "jhgan/ko-sroberta-multitask"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embedding_model = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )

    if os.path.exists(vectorstore_dir):

        from chromadb.config import Settings
        from chromadb import Client

        # DuckDBë¡œ ì„¤ì •
        settings = Settings(persist_directory="chroma_vectorstore", chroma_db_impl="duckdb")
        client = Client(settings)

        vectorstore = Chroma(client=client, persist_directory=vectorstore_dir, embedding_function=embedding_model)
    else:
        raise NotImplementedError()

    st.session_state["vectorstore_loaded"] = True

class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

if st.session_state["vectorstore_loaded"]:

    memory = st.session_state["memory"]

    retriever = vectorstore.as_retriever()

    OPENAI_KEY = st.secrets["general"]["openai_api_key"]
    llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0,
        openai_api_key=OPENAI_KEY,
        streaming=True
    )

    template = """ë‹¤ìŒê³¼ ê°™ì€ ë§¥ë½ì„ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.
    ë§Œì•½ ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³ ë§Œ ë§í•˜ê³  ë‹µì„ ì§€ì–´ë‚´ë ¤ê³  í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    ë‹¹ì‹ ì€ í•œêµ­ ì„¸ë²•ì— ëŒ€í•´ ì „ë¬¸ê°€ì¸ ì„¸ë¬´ì‚¬ì…ë‹ˆë‹¤.
    ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
    ë§¥ë½: {context}
    ì§ˆë¬¸: {question}
    ì´ì „ ëŒ€í™”: {chat_history}
    ë„ì›€ë˜ëŠ” ë‹µë³€:"""
    prompt = PromptTemplate(input_variables=["context", "question", "chat_history"], template=template)

    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=True,
        output_key="answer"
    )

    def stream_answer(question):
        answer_container = st.empty()
        callback_handler = StreamlitCallbackHandler(container=answer_container)
        result = qa({"question": question}, callbacks=[callback_handler])
        return result

    question_disabled = st.session_state["is_loading"]

    with st.form(key="question_form"):
        question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", 
            placeholder="ì˜ˆ: ì¢…í•©ì†Œë“ì„¸ëŠ” ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?", 
            disabled=question_disabled
        )

        submit_button_col, reset_button_col = st.columns([1, 1])
        with submit_button_col:
            submit_button = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°", use_container_width=True)
        with reset_button_col:
            reset_button = st.form_submit_button("ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True)

    if submit_button and not question_disabled:
        if question:
            st.session_state["is_loading"] = True

            with st.spinner("ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”..."):
                result = stream_answer(question)

            answer = result['answer']
            references = "\n\n".join([doc.page_content for doc in result['source_documents']])

            with st.expander("ğŸ“„ ì°¸ì¡°ëœ ë¬¸ì„œ"):
                st.write(f"**ì°¸ì¡°ëœ ë¬¸ì„œ:** {references}")

            st.session_state["chat_history"].append((question, answer))
            st.session_state["is_loading"] = False
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    if reset_button:
        with st.spinner("ëŒ€í™” ê¸°ë¡ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            st.session_state["chat_history"] = []

            success_message = st.empty() 
            success_message.success("ëŒ€í™” ë‚´ìš©ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            time.sleep(3) 
            success_message.empty()

    if st.session_state["chat_history"]:
        st.write("### ğŸ’¬ ëŒ€í™” ê¸°ë¡")
        for i, (q, a) in enumerate(st.session_state["chat_history"]):
            with st.expander(f"ì§ˆë¬¸ {i + 1}: {q}"):
                st.write(f"**ë‹µë³€:** {a}")
else:
    st.warning("ë¬¸ì„œ ë° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œë¥¼ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤...")
